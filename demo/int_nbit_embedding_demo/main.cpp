#include <iostream>
#include <vector>
#include <stdexcept>
#include <numeric> // For std::iota
#include <algorithm> // For std::generate, std::memcpy
#include <cstdlib> // For rand
#include <iomanip> // For std::fixed, std::setprecision

#include <ATen/ATen.h>
#include <cuda_runtime.h>
#include <ATen/cuda/CUDAContext.h> // For at::cuda::getCurrentCUDAStream()

#include "utils.h" // Our helper file with SparseType, quantization functions, etc.

// Forward declaration of the CUDA kernel wrapper function
// This signature must match the one generated by FBGEMM_GPU's codegen
// (found in the *_host_template.cu files)
extern "C" at::Tensor int_nbit_split_embedding_nobag_codegen_forward_unweighted_cuda(
    at::Tensor dev_weights,
    at::Tensor uvm_weights,
    at::Tensor weights_placements,
    at::Tensor weights_offsets,
    at::Tensor weights_tys,
    int64_t D, // const int64_t D, (nobag version)
    int64_t max_int2_D,
    int64_t max_int4_D,
    int64_t max_int8_D,
    int64_t max_float16_D,
    int64_t max_float32_D,
    at::Tensor indices,
    at::Tensor offsets,
    // const int64_t pooling_mode, (nobag version doesn't have this)
    int64_t row_alignment,
    // Tensor indice_weights, (unweighted version doesn't have this)
    int64_t output_dtype,
    at::Tensor lxu_cache_weights,
    at::Tensor lxu_cache_locations,
    int64_t max_float8_D,
    int64_t fp8_exponent_bits,
    int64_t fp8_exponent_bias);

// Helper to calculate row size, including scale/bias header
size_t get_quantized_row_size_bytes(int D, int N_bits) {
    if (N_bits != 2 && N_bits != 4 && N_bits != 8) {
        throw std::runtime_error("Unsupported N_bits in get_quantized_row_size_bytes");
    }
    return sizeof(uint16_t) * 2 + (D * N_bits + 7) / 8;
}


int main() {
    try {
        // --- Configuration ---
        const int N_bits = 4;         // Bit-width for quantization (INT4)
        const int D = 128;            // Embedding dimension
        const int num_embeddings = 20; // Number of embeddings in our table
        const int num_lookups = 5;   // Number of embeddings to look up
        const int64_t row_alignment = 16; // Typical row alignment for GPU
        const SparseType output_sparse_type = SparseType::FP32;

        std::cout << "--- Configuration ---" << std::endl;
        std::cout << "Bit-width (N): " << N_bits << std::endl;
        std::cout << "Dimension (D): " << D << std::endl;
        std::cout << "Num Embeddings: " << num_embeddings << std::endl;
        std::cout << "Num Lookups: " << num_lookups << std::endl;
        std::cout << "Output Type: FP32" << std::endl;
        std::cout << std::endl;

        // Use CUDA if available
        if (!at::cuda::is_available()) {
            std::cerr << "CUDA is not available. Exiting." << std::endl;
            return 1;
        }
        at::Device device(at::kCUDA);
        at::TensorOptions byte_options = at::TensorOptions().dtype(at::kByte).device(device);
        at::TensorOptions int_options = at::TensorOptions().dtype(at::kInt).device(device);
        at::TensorOptions long_options = at::TensorOptions().dtype(at::kLong).device(device);
        at::TensorOptions host_byte_options = at::TensorOptions().dtype(at::kByte);
        at::TensorOptions host_float_options = at::TensorOptions().dtype(at::kFloat);
        at::TensorOptions host_long_options = at::TensorOptions().dtype(at::kLong);


        // --- Data Preparation (CPU) ---
        std::cout << "--- Data Preparation (CPU) ---" << std::endl;
        std::vector<float> original_embedding_table_float(num_embeddings * D);
        // Fill with some predictable data (e.g., 0, 1, 2, ... D-1, then D, D+1, ...)
        for (int i = 0; i < num_embeddings; ++i) {
            for (int j = 0; j < D; ++j) {
                original_embedding_table_float[i * D + j] = static_cast<float>((i % 5) * D + j); // Keep values in a manageable range
            }
        }

        std::vector<uint8_t> all_quantized_data_cpu;
        size_t single_quantized_row_bytes = get_quantized_row_size_bytes(D, N_bits);
        
        std::cout << "Quantizing embedding table..." << std::endl;
        for (int i = 0; i < num_embeddings; ++i) {
            std::vector<uint8_t> q_row = quantize_row_nbit(original_embedding_table_float.data() + i * D, D, N_bits);
            if (q_row.size() != single_quantized_row_bytes) {
                 std::cerr << "Error: Quantized row " << i << " size mismatch. Expected: "
                           << single_quantized_row_bytes << ", Got: " << q_row.size() << std::endl;
                 // This might happen if quantize_row_nbit doesn't pad to alignment,
                 // but the kernel expects alignment. For this demo, we assume quantize_row_nbit gives
                 // the raw packed size including header. The kernel uses row_alignment for its internal addressing.
            }
            all_quantized_data_cpu.insert(all_quantized_data_cpu.end(), q_row.begin(), q_row.end());
        }
        std::cout << "Total size of quantized dev_weights data (CPU): " << all_quantized_data_cpu.size() << " bytes." << std::endl;


        std::vector<int64_t> lookup_indices_cpu(num_lookups);
        // Pick some indices to lookup, e.g., 0, num_embeddings/2, num_embeddings-1
        if (num_lookups > 0) lookup_indices_cpu[0] = 0;
        if (num_lookups > 1) lookup_indices_cpu[1] = num_embeddings / 2;
        if (num_lookups > 2) lookup_indices_cpu[2] = num_embeddings - 1;
        if (num_lookups > 3) lookup_indices_cpu[3] = 1;
        if (num_lookups > 4) lookup_indices_cpu[4] = num_embeddings / 3;
        // Fill remaining with other valid indices if num_lookups > 5
        for(int i=5; i<num_lookups; ++i) lookup_indices_cpu[i] = i % num_embeddings;


        // --- Tensor Creation (GPU) ---
        std::cout << "\n--- Tensor Creation (GPU) ---" << std::endl;
        at::Tensor dev_weights = at::empty({(int64_t)all_quantized_data_cpu.size()}, host_byte_options)
                                   .pin_memory() // Pin for faster H2D copy
                                   .to(device, /*non_blocking=*/true);
        std::memcpy(dev_weights.data_ptr<uint8_t>(), all_quantized_data_cpu.data(), all_quantized_data_cpu.size());
        
        // Assuming a single embedding table for simplicity
        at::Tensor weights_placements = at::tensor({static_cast<int32_t>(PlacementType::DEVICE)}, int_options);
        at::Tensor weights_offsets = at::tensor({0L}, long_options); // Offset for the single table is 0
        
        SparseType table_sparse_type;
        if (N_bits == 2) table_sparse_type = SparseType::INT2;
        else if (N_bits == 4) table_sparse_type = SparseType::INT4;
        else if (N_bits == 8) table_sparse_type = SparseType::INT8;
        else { throw std::runtime_error("N_bits not supported for table_sparse_type"); }
        at::Tensor weights_tys = at::tensor({static_cast<uint8_t>(table_sparse_type)}, byte_options);

        at::Tensor indices = at::tensor(lookup_indices_cpu, host_long_options).pin_memory().to(device, true);
        // For nobag, offsets define ranges of indices for each table.
        // If one table, it's [0, num_indices_for_this_table]
        at::Tensor offsets = at::tensor({0L, (long)num_lookups}, host_long_options).pin_memory().to(device, true);

        // Empty tensors for optional features
        at::Tensor uvm_weights = at::empty({0}, byte_options);
        at::Tensor lxu_cache_weights = at::empty({0}, byte_options);
        at::Tensor lxu_cache_locations = at::empty({0}, int_options);

        // --- Parameter Setup for Kernel Call ---
        int64_t max_int2_D_param = (N_bits == 2) ? D : 0;
        int64_t max_int4_D_param = (N_bits == 4) ? D : 0;
        int64_t max_int8_D_param = (N_bits == 8) ? D : 0;
        int64_t max_float16_D_param = 0;
        int64_t max_float32_D_param = 0;
        int64_t max_float8_D_param = 0;
        int64_t fp8_exp_bits = 0;
        int64_t fp8_exp_bias = 0;
        int64_t output_dtype_param = static_cast<int64_t>(output_sparse_type);


        // --- Invoke CUDA Kernel ---
        std::cout << "\n--- Invoking CUDA Kernel ---" << std::endl;
        at::cuda::synchronize(); // Ensure all data transfers are complete

        at::Tensor output_tensor = int_nbit_split_embedding_nobag_codegen_forward_unweighted_cuda(
            dev_weights, uvm_weights, weights_placements, weights_offsets, weights_tys,
            D, // Kernel's D parameter
            max_int2_D_param, max_int4_D_param, max_int8_D_param,
            max_float16_D_param, max_float32_D_param,
            indices, offsets,
            row_alignment,
            output_dtype_param,
            lxu_cache_weights, lxu_cache_locations,
            max_float8_D_param, fp8_exp_bits, fp8_exp_bias);

        at::cuda::synchronize(); // Ensure kernel execution is complete
        std::cout << "Kernel invocation complete." << std::endl;

        // --- Retrieve Output & Verification ---
        std::cout << "\n--- Verification ---" << std::endl;
        at::Tensor output_cpu = output_tensor.to(at::kCPU);
        float* output_data_ptr = output_cpu.data_ptr<float>();

        std::cout << std::fixed << std::setprecision(4);
        bool all_correct = true;
        for (int i = 0; i < num_lookups; ++i) {
            int64_t original_idx = lookup_indices_cpu[i];
            const uint8_t* p_quantized_row_data = all_quantized_data_cpu.data() + original_idx * single_quantized_row_bytes;
            
            std::vector<float> expected_dequantized_row = dequantize_row_nbit(p_quantized_row_data, D, N_bits);
            
            std::cout << "\nLookup " << i << " (Original Index: " << original_idx << "):" << std::endl;
            bool current_lookup_correct = true;
            for (int j = 0; j < D; ++j) {
                float expected_val = expected_dequantized_row[j];
                float actual_val = output_data_ptr[i * D + j]; // Output is [num_lookups][D]
                float diff = std::abs(expected_val - actual_val);
                if (diff > 1e-3) { // Tolerance for float precision issues (esp. with half)
                    if (j < 5 || j > D - 5) { // Print only first/last few differing values
                        std::cout << "  Dim " << j << ": Expected " << expected_val
                                  << ", Got " << actual_val << ", Diff " << diff << " <--- MISMATCH" << std::endl;
                    }
                    current_lookup_correct = false;
                    all_correct = false;
                } else {
                     if (j < 5 || j > D-5 ) { // Print first/last few matching values for inspection
                         std::cout << "  Dim " << j << ": Expected " << expected_val
                                   << ", Got " << actual_val << ", Diff " << diff << std::endl;
                     }
                }
            }
             if(current_lookup_correct && D > 10) {
                std::cout << "  ... (middle dimensions match) ..." << std::endl;
             }
             if (!current_lookup_correct && D > 10) {
                 std::cout << "  ... (mismatches in middle dimensions might exist) ..." << std::endl;
             }
        }

        if (all_correct) {
            std::cout << "\nSUCCESS: All retrieved embeddings match expected dequantized values." << std::endl;
        } else {
            std::cout << "\nFAILURE: Some retrieved embeddings DO NOT match expected dequantized values." << std::endl;
        }

    } catch (const std::exception& e) {
        std::cerr << "An exception occurred: " << e.what() << std::endl;
        return 1;
    }
    return 0;
}
